---
title: "exercises2-connor-gilmore"
output: pdf_document
---

```{r setup, echo=FALSE, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, echo=FALSE, include=FALSE}
library(ggplot2)
library(dplyr)
library(ggthemes)
library(ggforce)
library(tidyr)
library(quantmod)
library(mosaic)
library(foreach)
library(purrr)
library(cluster)
library(factoextra)
library(dendextend)
library(tm) 
library(tidyverse)
library(slam)
library(proxy)
library(class)
#library(Rcpp)
library(text2vec)
#library(data.table)
#library(magrittr)
library(superml)
#library(caret)
library(plyr)
library(SnowballC)
library(glmnet)
library(rpart)
library(naivebayes)
```

# Visual story telling part 1: green buildings 

```{r, echo=FALSE, include=FALSE}
gb_data <- read.csv('greenbuildings.csv')
```

```{r, echo=FALSE, include=FALSE}
#for each cluster, compare green-building rent to median of non-green buildings rent
rent_comp <- gb_data %>%
  select(CS_PropertyID, cluster, cluster_rent, Rent, green_rating) %>%
  group_by(cluster) %>% 
  filter(green_rating == 1) %>%
  mutate(green_comp = ifelse(Rent > cluster_rent, 'Green Building Rent Is Higher Than Cluster', 
                             'Green Building Rent Is Lower Than Cluster'))
```

```{r, echo=FALSE, include=FALSE}
#for each cluster, compare green-building rent to median of non-green buildings rent
non_gb_med <- gb_data %>%
  select(CS_PropertyID, cluster, cluster_rent, Rent, green_rating) %>%
  group_by(cluster) %>%
  filter(green_rating == 0) %>%
  mutate(comp_rent = median(Rent))
```

```{r, echo=FALSE, include=FALSE}
#for each cluster, compare green-building rent to median of non-green buildings rent
new_gb_df <- rent_comp %>%
  bind_rows(non_gb_med) %>%
  mutate(comp_rent = ifelse(is.na(comp_rent), Rent, comp_rent)) %>%
  arrange(cluster) %>%
  select(cluster, green_rating, comp_rent) %>%
  distinct(cluster, green_rating, comp_rent, .keep_all = TRUE)

comp_rent_green <- new_gb_df %>% filter(green_rating == 1)
comp_rent_nongreen <- new_gb_df %>% filter(green_rating == 0)
comp_rent_df <- comp_rent_green %>%
  left_join(comp_rent_nongreen, by='cluster', suffix=c('_green', '_non_green')) %>%
  mutate(comp_result = ifelse(comp_rent_green > comp_rent_non_green, 'Green Building Rent Is Higher', 'Non-Green Building Rent Is Higher')) %>%
  select(cluster, comp_result) %>%
  filter(is.na(comp_result) == FALSE)
```


When looking at comparing rent of green buildings to non-green buildings, it is important to account for comparisons within cluster so that we adjust for relevant information that locality may play in rent prices. 

First, we look to see how green buildings' rents compare to non-green buildings' rents. Adjusting for cluster information, we that for the majority of clusters (67% of them), the green buildings have higher rent than the median value of the non-green buildings within their respective cluster.  

```{r, echo=FALSE}
#for each cluster, compare green-building rent to non-green buildings rent
ggplot(comp_rent_df, aes(comp_result, 
                         fill=factor(ifelse(comp_result=="Green Building Rent Is Higher","Highlighted","Normal")))) +
  geom_bar(stat='count') +
  scale_fill_manual(name = "comp_result", values=c("green4","grey50")) +
  labs(title="In 67% of Clusters, Green Buildings Have Higher Rent", 
       subtitle="a comparison of rent w/in clusters, adjusting for possible confounding cluster information",
       y='frequency', 
       x='count of clusters where green or non-green building has higher rent') +
  theme(legend.position = "none", 
        panel.background = element_blank(),
        panel.border = element_blank(),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        # Change axis line
        axis.line = element_line(colour = "black"), 
        plot.title = element_text(face = "bold"))
```

However, the green building status is not the only thing that suggests higher rent. So, we turn to investigating other key factors that are confounding variables.  

We may hypothesize that the number of stories may be influencing green-building rent to be higher, so we can control for this as a confounding variable by restricting our comparison groups (green buildings and non-green buildings) to the number of stories in the building we are developing, 15, which we see that green-building rent is still higher.

```{r, echo=FALSE, include=FALSE}
gb_data$green_rating_label <- ifelse(gb_data$green_rating == 1, 'green', 'non-green')
```

```{r, echo=FALSE, include=FALSE}
med_green_15 <- gb_data %>%
  filter(green_rating_label == 'green') %>%
  filter(stories == 15)
med_green_15_rent <- median(med_green_15$Rent)

med_nongreen_15 <- gb_data %>%
  filter(green_rating_label == 'non-green') %>%
  filter(stories == 15)
med_nongreen_15_rent <- median(med_nongreen_15$Rent)

gb_data_15 <- gb_data %>%
  filter(stories == 15)
```

```{r, echo=FALSE}
#age
ggplot(gb_data_15, aes(x=stories, y=Rent, 
                       fill=factor(ifelse(green_rating_label=="green","Highlighted","Normal")))) +
  geom_boxplot() +
  scale_fill_manual(name = "green_rating_label", values=c("green4","grey50")) +
  scale_x_continuous(breaks = c(14.8, 15.2), 
                     labels = c("Green Buildings", "Non-Green Buildings")) + 
  labs(title="Green Buildings With 15 Stories Have A Higher Median Rent", 
       subtitle="a comparison of rent, adjusting for possible confounding information in the number of stories",
       y='rent value', 
       x='buildings with 15 stories') +
  theme(legend.position = "none",
      panel.background = element_blank(),
      panel.border = element_blank(),
      panel.grid.major = element_blank(),
      panel.grid.minor = element_blank(),
      # Change axis line
      axis.line = element_line(colour = "black"), 
      plot.title = element_text(face = "bold"),
      #axis.text.x=element_blank(),
      #axis.ticks.x=element_blank()
      ) +
  annotate(
    geom = "curve", x = 14.9, y = 46, xend = 14.81, yend = 38, 
    curvature = .3, arrow = arrow(length = unit(2, "mm"))) +
  annotate(geom = "text", x = 14.91, y = 46.9, label = "Median Green Rent: $36.95", 
           hjust = "left", color='green4', fontface=2) +
  annotate(
    geom = "curve", x = 15.16, y = 15, xend = 15.19, yend = 24, 
    curvature = .3, arrow = arrow(length = unit(2, "mm"))) +
  annotate(geom = "text", x = 14.91, y = 13, label = "Median Non-Green Rent: $24.36", 
           hjust = "left", fontface=2)
```


Next, we check to make sure that the size of the building is not a confounding variable, by again restricting our comparison to buildings in a similar square footage range. Here, we start to see the initial analysis that green rent is higher come apart. 

```{r, echo=FALSE, include=FALSE}
gb_data_size <- gb_data %>%
  filter(size >= 245000 & size <= 255000)
gb_data_size_finer <- gb_data %>%
  filter(size >= 249000 & size <= 251000)
gb_data_size_g <- gb_data %>% 
  filter(green_rating_label == 'green')
gb_data_size_ng <- gb_data_size %>% 
  filter(green_rating_label == 'non-green')
gb_data_size_g_rent <- median(gb_data_size_g$Rent)
gb_data_size_ng_rent <- median(gb_data_size_ng$Rent) 
```

```{r, echo=FALSE}
#age
ggplot(gb_data_size, aes(x=size, y=Rent, color=factor(green_rating_label))) +
  geom_point() + 
  scale_fill_manual(values=c("green4","grey50")) +
  scale_color_manual(values=c("green4","grey50")) +
  geom_smooth(method = 'lm', se = FALSE) +
  labs(title="However, Adjusting For Similar Size, Green Buildings Have A Lower Rent", 
       subtitle="a comparison of rent, adjusting for possible confounding information in the building size",
       y='rent value', 
       x='green and non-green buildings size - around 250k sqft (+/- 5k sqft)') +
  theme(legend.position = "none",
        panel.background = element_blank(),
        panel.border = element_blank(),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        # Change axis line
        axis.line = element_line(colour = "black"), 
        plot.title = element_text(face = "bold"))  +
  annotate(
    geom = "curve", x = 251000, y = 13.5, xend = 250400, yend = 26.5,
    curvature = .3, color='green4', arrow = arrow(length = unit(2, "mm"))) +
  annotate(geom = "text", x = 250000, y = 12, label = 'Median Green Rent Is $6.15 Lower', 
           hjust = "left", color='green4', fontface=2)
```

```{r, echo=FALSE, include=FALSE}
gb_data_age <- gb_data %>%
  filter(leasing_rate >= 0.90) %>% 
  filter(age < 40) %>% 
  filter(renovated == 0) %>% 
  filter(net == 0) %>% 
  group_by(green_rating_label, age) %>% 
  summarise(median_rent = median(Rent, na.rm = TRUE))

gb_data_age_gb <- gb_data_age %>% 
  filter(green_rating_label == 'green')

gb_data_age_ngb <- gb_data_age %>% 
  filter(green_rating_label == 'non-green')

gb_data_age_comp <- gb_data_age_gb %>% 
  left_join(gb_data_age_ngb, by='age', suffix=c('_green', '_non_green')) %>% 
  mutate(green_comp = ifelse(median_rent_green > median_rent_non_green, 
                             'green is higher', 'green is lower')) %>% 
  filter(green_comp == 'green is higher')
```

Finally, we turn to the age, which is part of the analyst's calculation. I adjusted for two possible confounding variables: if the building had a renovation (a future cost and hit to profitability) and if the building has tenants pay utility costs (making the rent lower). I considered only buildings that did not have a renovation and whose tenants did not rent on a net contract basis. Here again, we see no conclusive evidence green buildings have a higher rent over time, or even half the time. 

```{r, echo=FALSE}
#age
ggplot(gb_data_age, aes(x=age, y=median_rent, color=factor(green_rating_label))) + 
  geom_point() + 
  geom_line() + 
  #geom_smooth(se=FALSE) + 
  scale_fill_manual(values=c("green4","grey50")) +
  scale_color_manual(values=c("green4","grey50")) + 
  #scale_x_continuous(n.breaks=35) + 
  labs(title="Finally, Green Buildings Have Lower Rents Over Half the Years It's Around", 
       subtitle="a comparison of rent, adjusting for possible confounding information in the building age",
       y='median rent value', 
       x='building age') +
  theme(legend.title = element_blank(),
        panel.background = element_blank(),
        panel.border = element_blank(),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        # Change axis line
        axis.line = element_line(colour = "black"), 
        plot.title = element_text(face = "bold")) 
```

To summarize, at first glance the green building's rent looks higher and seems promising even adjusting for confounding variables in cluster information and stories. However, when we adjust for confounding variables such as size and age (not including renovated and net-contracted buildings), we see that green buildings do not have an advantage over non-green buildings. On those grounds, I disagree with the analysts conclusion.





# Visual story telling part 2: 

```{r, echo=FALSE, include=FALSE}
rm(list=ls())
```

```{r, echo=FALSE, include=FALSE}
abia_data <- read.csv('ABIA.csv')
```

```{r, echo=FALSE, include=FALSE}
abia_df <- abia_data %>% 
  mutate(part_of_week = ifelse(DayOfWeek == 6 | DayOfWeek == 7, 'weekend', 'week')) %>% 
  mutate(real_delay = ArrDelay - DepDelay) %>% 
  mutate(total_taxi_time = TaxiIn + TaxiOut) %>% 
  mutate(arr_time_of_day = case_when(ArrTime >= 0 & ArrTime < 700 ~ 'early morning',
                                     ArrTime >= 700 & ArrTime < 1200 ~ 'morning proper',
                                     ArrTime >= 1200 & ArrTime < 1400 ~ 'early afternoon',
                                     ArrTime >= 1400 & ArrTime < 1700 ~ 'late afternoon',
                                     ArrTime >= 1700 & ArrTime < 2100 ~ 'evening',
                                     ArrTime >= 2100 ~ 'night')) %>% 
  mutate(dep_time_of_day = case_when(DepTime >= 0 & DepTime < 700 ~ 'early morning',
                                     DepTime >= 700 & DepTime < 1200 ~ 'morning proper',
                                     DepTime >= 1200 & DepTime < 1400 ~ 'early afternoon',
                                     DepTime >= 1400 & DepTime < 1700 ~ 'late afternoon',
                                     DepTime >= 1700 & DepTime < 2100 ~ 'evening',
                                     DepTime >= 2100 ~ 'night')) %>% 
  mutate(austin_dep_or_arr = ifelse(Origin == 'AUS', 'austin departure', 'austin arrival')) %>% 
  mutate(perc_time_dep_delay = DepDelay / (DepDelay + ActualElapsedTime)) %>% 
  mutate(perc_time_arr_delay = ArrDelay / (ArrDelay + ActualElapsedTime)) %>% 
  mutate(flight_speed = Distance / AirTime) %>% 
  mutate(distance_cat = ifelse(AirTime >= 180, 'long distance', 'short distance')) %>% 
  mutate(busy_bin = ntile(total_taxi_time, 5)) %>% 
  mutate(busy_level = case_when(busy_bin == 1 ~ 'hardly busy',
                                busy_bin == 2 ~ 'somewhat busy',
                                busy_bin == 3 ~ 'normal',
                                busy_bin == 4 ~ 'moderately busy',
                                busy_bin == 5 ~ 'very busy'))
```

```{r, echo=FALSE, include=FALSE}
tail_count <- abia_df %>% 
  group_by(TailNum) %>% 
  summarise(tail_flight_count = n())
```

```{r, echo=FALSE, include=FALSE}
tail_df <- abia_df %>% 
  filter(austin_dep_or_arr == 'austin arrival') %>% 
  #select(TailNum, part_of_week, total_taxi_time, real_delay) %>% 
  group_by(TailNum, part_of_week) %>% 
  summarise(total_taxi_time_med = median(total_taxi_time, na.rm=TRUE), 
            real_delay_med = median(real_delay, na.rm=TRUE)) %>% 
  filter(!is.na(total_taxi_time_med) & !is.na(real_delay_med))
```

```{r, echo=FALSE, include=FALSE}
diverging_df <- abia_df %>%
  filter(!is.na(real_delay)) %>% 
  filter(austin_dep_or_arr == 'austin arrival') %>% 
  group_by(busy_level) %>% 
  summarize(median_real_delay = median(real_delay))
diverging_df$busy_type <- ifelse(diverging_df$median_real_delay < 0, "below", "above")
level_order <- c('hardly busy', 'somewhat busy', 'normal', 'moderately busy', 'very busy')
```

```{r, echo=FALSE}
ggplot(diverging_df, aes(x=factor(busy_level, level=level_order), 
                                  y=median_real_delay, label=median_real_delay)) + 
  geom_point(stat='identity', aes(color=busy_type), size=6)  +
  geom_segment(aes(y = 0, 
                   x = busy_level, 
                   yend = median_real_delay, 
                   xend = busy_level,
                   color=busy_type)) + 
  scale_color_manual(name="Flight Time Difference", 
                     labels = c("Flight Went Shorter", "Flight Went Longer"), 
                     values = c("below"="#00ba38", "above"="#f8766d")) + 
  geom_text(color="white", size=2) +
  theme(legend.position = "none",
        panel.background = element_blank(),
        panel.border = element_blank(),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        # Change axis line
        #axis.title.y = element_blank(), 
        axis.line = element_line(colour = "black"), 
        plot.title = element_text(face = "bold"))  +
  #run longer annotation
  annotate(
    geom = "curve", x = 3.9, y = 5.8, xend = 4.9, yend = 5.3,
    curvature = .3, color='#f8766d', arrow = arrow(length = unit(2, "mm"))) +
  annotate(geom = "label", x = 3.5, y = 3.6, 
           label = 'at very busy times, flights\ntend to run 5 minutes longer', hjust = "left",
           color='#f8766d', size = 3, fontface=2)  + 
  #run shorter annotation
  #annotate(
    #geom = "curve", x = 1.5, y = 2, xend = 1.9, yend = .3,
    #curvature = .3, color='#00ba38', arrow = arrow(length = unit(2, "mm"))) +
  #annotate(
    #geom = "curve", x = 1.5, y = 2, xend = 1.1, yend = .3,
    #curvature = -.3, color='#00ba38', arrow = arrow(length = unit(2, "mm"))) +
  annotate(geom = "label", x = 1.5, y = -5.5, 
           label = "when it's not busy, flights\ntend to run 6 minutes shorter", 
           hjust = "left",
           color='#00ba38', size = 3, fontface=2)  + 
  ylim(-8, 8) +
  coord_flip() + 
  #geom_hline(yintercept = 0, color='grey') + 
  theme_fivethirtyeight(base_size = 10) +
  theme(
    #legend.position = "none",
    panel.grid.major.x = element_blank(),
    plot.caption = element_text(hjust = 0, face= "italic"), #Default is hjust=1
    plot.title.position = "plot", #NEW parameter. Apply for subtitle too.
    plot.caption.position =  "plot", 
    #axis.title.x = element_text()
    )+
  labs(
    title='Arriving In Austin At A Busy Time? Expect Your Flight to Run Long.',
    subtitle = 'how busy the Austin airport is vs how long (in median minutes) flights that are arriving into Austin run long', 
    y = 'median minutes flights run longer than expected',
    x = '',
    caption = "____\nNote: The airport levels of busy (y-axis), are proxy measured by the total taxi in and out time in minutes for a given flight. The idea being,\nthe longer it takes for a taxi to get in and out, usually the more cars and people are, i.e. the busier it is. The categorical levels represent\nfive bins of this data. How long a flight went over the expected length (x-axis) is the difference between the arrival delay and departure\ndelay to get a true sense of if the flight actually ran long or it was just delayed on arrival or departure."
  ) 
```





# Portfolio modeling

```{r, echo=FALSE, include=FALSE}
rm(list=ls())
```

```{r, echo=FALSE, include=FALSE}
#collect data
mystocks <- c("SHV", "XMLV", "SPLV", "MRGR",
              "QQQ", "VUG", "IWF", "XLY",
              "USMV", "VGT", "COMT", "AOM")
getSymbols(mystocks, from = "2016-08-08")
```

```{r, echo=FALSE, include=FALSE}
#data prep:
# adjust for splits and dividends
# combine close-to-close changes in a single matrix

#portofilo 1
SHVa <- adjustOHLC(SHV)
XMLVa <- adjustOHLC(XMLV)
SPLVa <- adjustOHLC(SPLV)
MRGRa <- adjustOHLC(MRGR)
port_1_returns <- cbind(ClCl(SHVa), ClCl(XMLVa), ClCl(SPLVa), ClCl(MRGRa))
port_1_returns <- as.matrix(na.omit(port_1_returns))

#portfolio 2
QQQa <- adjustOHLC(QQQ)
VUGa <- adjustOHLC(VUG)
IWFa <- adjustOHLC(IWF)
XLYa <- adjustOHLC(XLY)
port_2_returns <- cbind(ClCl(QQQa), ClCl(VUGa), ClCl(IWFa), ClCl(XLYa))
port_2_returns <- as.matrix(na.omit(port_2_returns))

#portfolio 3
USMVa <- adjustOHLC(USMV)
VGTa <- adjustOHLC(VGT)
COMTa <- adjustOHLC(COMT)
AOMa <-adjustOHLC(COMT)
port_3_returns <- cbind(ClCl(USMVa), ClCl(VGTa), ClCl(COMTa), ClCl(AOMa))
port_3_returns <- as.matrix(na.omit(port_3_returns))
```

```{r, echo=FALSE, include=FALSE}
#portfolio 1 - safe
#ETFs: SHV, XLV, SPLV

# Update the value of your holdings
# Assumes an equal allocation to each asset
initial_wealth = 100000
sim1 <- foreach(i=1:5000, .combine='rbind') %do% {
	total_wealth = initial_wealth
	weights = c(0.25, 0.25, 0.25, 0.25)
	holdings = weights * total_wealth
	n_days = 20 #4 weeks ~ 20 training day
	wealthtracker = rep(0, n_days)
	for(today in 1:n_days) {
		return.today = resample(port_1_returns, 1, orig.ids=FALSE)
		holdings = holdings + holdings*return.today
		total_wealth = sum(holdings)
		wealthtracker[today] = total_wealth
	}
	wealthtracker
}

port_1_var <- quantile(sim1[,20]- initial_wealth, prob=0.05)
port_1_avg_pl <- mean(sim1[, 20] - initial_wealth)
```

```{r, echo=FALSE, include=FALSE}
#portfolio 2 - aggressive
#ETFs: QQQ, VUG, IWF, XLY

# Update the value of your holdings
# Assumes an equal allocation to each asset
initial_wealth = 100000
sim2 <- foreach(i=1:5000, .combine='rbind') %do% {
	total_wealth = initial_wealth
	weights = c(0.25, 0.25, 0.25, 0.25)
	holdings = weights * total_wealth
	n_days = 20 #4 weeks ~ 20 training day
	wealthtracker = rep(0, n_days)
	for(today in 1:n_days) {
		return.today = resample(port_2_returns, 1, orig.ids=FALSE)
		holdings = holdings + holdings*return.today
		total_wealth = sum(holdings)
		wealthtracker[today] = total_wealth
	}
	wealthtracker
}

port_2_var <- quantile(sim2[,20]- initial_wealth, prob=0.05)
port_2_avg_pl <- mean(sim2[, 20] - initial_wealth)
```

```{r, echo=FALSE, include=FALSE}
#portfolio 3 - diverse 
#ETFs: USMV (low volatility), VGT (aggressive), COMT (moderate), AOM (moderate)

# Update the value of your holdings
# Assumes an equal allocation to each asset
initial_wealth = 100000
sim3 <- foreach(i=1:5000, .combine='rbind') %do% {
	total_wealth = initial_wealth
	weights = c(0.25, 0.25, 0.25, 0.25)
	holdings = weights * total_wealth
	n_days = 20 #4 weeks ~ 20 training day
	wealthtracker = rep(0, n_days)
	for(today in 1:n_days) {
		return.today = resample(port_3_returns, 1, orig.ids=FALSE)
		holdings = holdings + holdings*return.today
		total_wealth = sum(holdings)
		wealthtracker[today] = total_wealth
	}
	wealthtracker
}

port_3_var <- quantile(sim3[,20]- initial_wealth, prob=0.05)
port_3_avg_pl <- mean(sim3[, 20] - initial_wealth)
```



**Analysis Background**

For this task, three portfolios were constructed of $100,000 allocated across four ETFs within each portfolio and analyzed the short-term tail risk of the three portfolios. Each ETF across the portfolios has at least a five-year history, and bootstrap resampling (over five thousand iterations) was used on that five-year data (08/08/2016 – 08/08/2021) to calculate the Value at Risk (VaR) for each at the 5% level. The composition of each portfolio was based on the level of risk for the ETFs: safe, aggressive, and diversified. In this report below, you will find a summary of each portfolio and the VaR findings, followed by a concluding comparison across all three. 

**Portfolio 1 – Safe**

The first portfolio is made up of four safe ETFs: SHV (iShares Short Treasury Bond ETF), XMLV (Invesco S&P MidCap Low Volatility ETF), SPLV (Invesco S&P 500 Low Volatility ETF), and MRGR (ProShares Merger ETF). These were selected using the ETF Database groupings (e.g., low volatility). These ETFs have relatively stable share prices, so one would expect the VaR to be low. However, we also know that these ETFs lag in bull markets, so it is not always the case the VaR will be low. After performing bootstrap resampling for the 20-trading day result (over five thousand iterations), this portfolio has a VaR of \$4130.95 and an average profit of \$544.67, the lowest across all three portfolios. 

**Portfolio 2 – Aggressive **

The second portfolio is made up of four aggressive ETFs: QQQ (Invesco QQQ Trust), VUG (Vanguard Growth ETF), IWF (iShares Russell 1000 Growth ETF), and XLY (Consumer Discretionary Select Sector SPDR Fund). These were selected using the ETF Database groupings (e.g., aggressive). These types of ETFs are known to be high risk/high reward to provide growth. Given this, we might expect the VaR to be high for this portfolio. After performing bootstrap resampling for the 20-trading day result (over five thousand iterations), we see this expectation hold as the portfolio has a VaR of \$7947.62, almost twice that of portfolio 1 (safe). The average profit of this portfolio is at \$1930.84, about 3.5 times that of profolio 1. This, whiwth the high VaR, indicates that this portfolio is indeed, high risk / high reward. 

**Portfolio 3 – Diversified **

The second portfolio is made up four ETFs of different risk levels. One is low volatility: USMV (iShares MSCI USA Min Vol Factor ETF); two are moderate: COMT (iShares GSCI Commodity Dynamic Roll Strategy ETF) and AOM (Shares Core Moderate Allocation ETF); and one is aggressive: VGT (Vanguard Information Technology ETF). These were selected using the ETF Database groupings (e.g., low volatility, moderate, and aggressive). The diversification of risk levels should, in theory, put the VaR somewhere between the safe portfolio and diversified portfolio. After performing bootstrap resampling for the 20-trading day result (over five thousand iterations), this theory holds as this portfolio has a VaR of \$6100.14 and an average profit of \$1219.93, almost halfway between portfolio 1 (safe) and portfolio 2 (aggressive).

**Conclusions**

By conducting bootstrap sampling, we can better estimate the VaR for each of the portfolios (safe, aggressive, and diversified), thus better quantifying uncertainty around investment decisions. As discussed above, each portfolio returns a different VaR at the 5% level. Here, we see that the first portfolio, the safe portfolio, returns the lowest VaR and average profit, which might best appeal to investors who want to safeguard their investment. In contrast, the second portfolio returns the highest VaR, but also highest average profit, which might best appeal to investors who are looking for high risk/high reward investments. And for those investors with a risk aversion level in between, the third portfolio is the better option, returning a VaR and average profit between the first and third portfolio.


# Market Segmentation

```{r, echo=FALSE, include=FALSE}
rm(list=ls())
```

```{r, echo=FALSE, include=FALSE}
sm_data <- read.csv('social_marketing.csv')
```

```{r, echo=FALSE, include=FALSE}
#variable selection 
#find percentage of zeros in a given interest
sum(sm_data$small_business == 0 ) / length(sm_data$small_business)

#drop regardless of other pre-processing steps
always_drops <- c('X', 'adult', 'spam', 'uncategorized')
```

```{r, echo=FALSE, include=FALSE}
#drop varaibles above
sm_df <- sm_data[, -which(names(sm_data) %in% always_drops)]
```

```{r, echo=FALSE, include=FALSE}
#data pre-processing

#first normalize phrase counts to phrase frequencies.
X <- sm_df/rowSums(sm_df)
X <- scale(X, center=TRUE, scale=TRUE)
distance_between_customers <- dist(X)
```

```{r, echo=FALSE, include=FALSE}
#hierarchical clustering
#build hierarchical cluster model 
h1 <- hclust(distance_between_customers, method='complete')
```

```{r, echo=FALSE, include=FALSE}
#run to get wss statistic
p1 <- factoextra::fviz_nbclust(X, FUN = hcut, method = "wss")
```

```{r, echo=FALSE, include=FALSE}
#run to get gap statistic
# about 2 hour run time
#p2 <- factoextra::fviz_nbclust(X, FUN = hcut, method = "gap_stat", nboot=50)
```

```{r, echo=FALSE, include=FALSE}
#pick four clusters as that is more of a relevant number for marketing needs, than the 10 indicated by the wss and gap statistics 
four_clusters <- cutree(h1, k=4)
```

```{r,echo=FALSE, include=FALSE}
# get counts and proportions of clusters
summary(factor(four_clusters))
(summary(factor(four_clusters)) / nrow(sm_df)) * 100
```

```{r, echo=FALSE, include=FALSE}
#get props instead of counts
prop_df <- sm_df/rowSums(sm_df)
#separate the clusters into separate dataframes to get summary statistics
cluster1_df <- prop_df[which(four_clusters == 1), ]
cluster2_df <- prop_df[which(four_clusters == 2), ]
cluster3_df <- prop_df[which(four_clusters == 3), ]
cluster4_df <- prop_df[which(four_clusters == 4), ]
```

```{r, echo=FALSE, include=FALSE}
#get medians for interpretation
clust_1_vals <- data.frame(apply(cluster1_df, 2, median))
colnames(clust_1_vals) <- ('median_vals')
clust_2_vals <- data.frame(apply(cluster2_df, 2, median))
colnames(clust_2_vals) <- ('median_vals')
clust_3_vals <- data.frame(apply(cluster3_df, 2, median))
colnames(clust_3_vals) <- ('median_vals')
clust_4_vals <- data.frame(apply(cluster4_df, 2, median))
colnames(clust_4_vals) <- ('median_vals')
```

```{r, echo=FALSE, include=FALSE}
#get medians for interpretation

#cluster 1
clust_1_vals %>% 
  filter(median_vals > 0) %>% 
  mutate(full_perc = median_vals*100) %>%
  arrange(desc(median_vals))

#cluster 2
clust_2_vals %>% 
  filter(median_vals > 0) %>% 
  mutate(full_perc = median_vals*100) %>%
  arrange(desc(median_vals))

#cluster 3
clust_3_vals %>% 
  filter(median_vals > 0) %>% 
  mutate(full_perc = median_vals*100) %>%
  arrange(desc(median_vals))

#cluster 4
clust_4_vals %>% 
  filter(median_vals > 0) %>% 
  mutate(full_perc = median_vals*100) %>%
  arrange(desc(median_vals))
```


**Market Segmentation Overview**

For this analysis, the tweets of NutrientH20’s Twitter followers were mined to understand NutrientH20’s market segments and what they talk about on Twitter. There are four market segments derived from the analysis named: Everyday Life Tweeters, Small Business Group, Active Naturalists, and Tech Savvy Politicos. This analysis will help guide NutrientH20 to craft more effective social media messaging for improved engagement across these four segments. 

**Analysis Background**

*Pre-Processing and Model Building* 

For this analysis, a market segment is defined as a cluster. An agglomerative hierarchical clustering method was applied to the tweet data (where the data represents a count of different interests a given user discusses, as annotated by human reviewers). First, there are three interests left out: uncategorized, spam, and adult. This is because they do not offer much for social media managers to craft better messages and because of the rarities of occurrences and known inaccuracies in human annotation. Then, the count data of all the other interests was normalized to interest frequencies (i.e. proportions) instead of pure interest count and then scaled and centered. In the hierarchical clustering algorithm, the complete linkage method was used because it is more robust to outliers. This is important because there are many inaccuracies with the human reviewers who annotated the tweets, so the complete linkage function can help to keep the outliers’ adverse influence at bay. 

*Cluster Selection*

An elbow plot was generated for the WSS and Gap Statistic for this clustering method over a range of 10 cluster (k) counts to help select the number of clusters. These both generated an optimal cluster count of 10, however this does not make sense in the context of marketing and market segmentation – 10 is too many to become familiar with and use. So, a cluster count of four was chosen, meaning there are four market segments. This is much more feasible for social media managers at NutrientH20 to learn, use, and keep track of. The median values of frequencies of interests for each cluster were generated and synthesized in the interpretations in the sections describing the market segments below. 

**Segment 1 – Everyday Life Tweeters (95.2%)**

This segment is by far the largest of NutrientH20’s market segments in Twitter at 95.2%. The brand followers here tweet about everyday life interests from general chatter to photo sharing to sports and more. In fact, on median, these followers mostly discuss general chatter at 10% followed by photo sharing at 5.6% and current events at 3.6%. These followers, though, have a long tail of interest discussion at very similar frequencies. This includes travel, sports, food, cooking, shopping, and more. From this we infer that these brand followers are interested in tweeting about broad interests that come up in the course of everyday life. For NutrientH20’s social media managers, tweets that resonate with everyday life interests, will resonate with this segment. This segment does not have a very specific, defined set of interests, so it is important for NutrientH20’s social media managers to not craft too interest-specific tweets to try to reach and engage these followers.

**Segment 2 – Small Business Group (2.8%)**

This segment, although small, is a much more interest-focused market segment than the first segment, Everyday Life Tweeters. This segment talks a lot about small business, on median at 6.7% and current events at 6.25%, indicating these tweeters are focused on small businesses and the world and markets they operate in. They are also sharing photos along the way, possibly about their small business or events affecting them, on median at 7.4%. What is interesting about this group is that they mainly are just discussing general chatter at a median 16.7%. What this might tell us is that while these followers may post musings or random thoughts a lot, when they do discuss interests, they are tweeting, with images, about small businesses and current events. For social media managers at NutrientH20, this means these followers may not be hyper-focused all the time on more niche-interests, but when they do, it’s on small businesses and current events. So, they will relate more and engage more with those tweets. Maybe if NutrientH20 tweets about their small businesses, or discusses potential small business partnership, they can improve engagement and following among this group. 

**Segment 3 – Active Naturalists (1.69%)**

This segment accounts for a small portion of NutrientH20’s brand followers, however it is a much more interest-focused than the first, more broad segment: Everyday Life Tweeters. This group, on median spends 7.7% of their time on eco interests, 4% on travel, and 6.8% on sharing photos, presumably related to those two interests. While they spend most of their tweets on chatter, on median 19% of the time, this does not mean they are not interested in active naturalist-related interests. What it means is that most of their tweets may just be random musings, but when they do talk about specific interests, they talk about eco, travel, and photos thereof. This is very helpful for NutrientH20’s social media managers. They can use this to make nature /eco-related tweets (not all of the time though), so these followers will engage.

**Segment 4 – Tech Savvy Politicos (0.33%)**

The smallest and one of the most interest-focused market segments. This market segment spends on median, 15.2% of their tweets on computers and 10.6% on politics. How can these two interests be related? With the rise of Big Tech, increasing regulations, and the relationship between social media platforms and election campaigning, tech and politics are increasingly intertwined. What does this mean for NutrientH20’s social media managers? It is important to not craft tweets that may create controversies on these issues, but a safer way to engage may be to stay away from politics, but every so often highlight the company’s use of technology.

**Conclusions**

These four market segments, varied in size, represent the market segments that NutrientH20’s social media managers and marketing professionals need to learn, use, and understand to craft more effective social media messaging. For most of their users (95.2%), tweets that resonate with everyday life are going to resonate the most, but for the other market segments, it is important to craft messaging that resonates to the specific interests they have: small businesses and related events, being active and in nature, and tech-related political topics. These market segments were discovered via an agglomerative hierarchical clustering, using the complete linkage function on normalized and scaled distance data. The number of segments (i.e. clusters) were chosen based on domain relevance, as the WSS and Gap statistics determined a high number of clusters, which would be unhelpful for NutrientH20’s social media managers. Over time, the data can be updated with new followers and the clustering re-ran to discover what new (or maybe un-changed) clusters appear. 



# Author attribution 

```{r, echo=FALSE, include=FALSE}
rm(list=ls())
```

```{r, echo=FALSE, include=FALSE}
readerPlain = function(fname){
				tm::readPlain(elem=list(content=readLines(fname)), 
							id=fname, language='en') }
```

```{r, echo=FALSE, include=FALSE}
#get list of directories and 
train_path <- '/Users/connorgilmore/dev/sta-380/STA380/data/ReutersC50/C50train'
train_dirs <- list.dirs(train_path, recursive = TRUE)
train_files <- list.files(train_path, recursive=TRUE, full.names=TRUE)
```

```{r, echo=FALSE, include=FALSE}
#get list of directories and 
test_path <- '/Users/connorgilmore/dev/sta-380/STA380/data/ReutersC50/C50test'
test_dirs <- list.dirs(test_path, recursive = TRUE)
test_files <- list.files(test_path, recursive=TRUE, full.names=TRUE)
```

```{r, echo=FALSE, include=FALSE}
#get author names - the response variable
train_author_names <- list()
for(filename in train_files) {
  author_l <- strsplit(filename, "/")
  author <- tail(author_l[[1]], 2)[1]
  train_author_names <- append(train_author_names, author)
}
```

```{r, echo=FALSE, include=FALSE}
#get author names - the response variable
test_author_names <- list()
for(filename in test_files) {
  author_l <- strsplit(filename, "/")
  author <- tail(author_l[[1]], 2)[1]
  test_author_names <- append(test_author_names, author)
}
```

```{r, echo=FALSE, include=FALSE}
all_train <- lapply(train_files, readerPlain) 

mynames_train <- train_files %>%
  { strsplit(., '/', fixed=TRUE) } %>%
  { lapply(., tail, n=2) } %>%
  { lapply(., paste0, collapse = '') } %>%
  unlist

names(all_train) <- mynames_train
```

```{r, echo=FALSE, include=FALSE}
all_test <- lapply(test_files, readerPlain) 

mynames_test <- test_files %>%
  { strsplit(., '/', fixed=TRUE) } %>%
  { lapply(., tail, n=2) } %>%
  { lapply(., paste0, collapse = '') } %>%
  unlist

names(all_test) <- mynames_test
```

```{r, echo=FALSE, include=FALSE}
train_df <- do.call(rbind, Map(data.frame, author_name=train_author_names, file_id=mynames_train))
colnames(train_df) <- c('author_name', 'file_id')
train_df$type <- 'train'
```

```{r, echo=FALSE, include=FALSE}
test_df <- do.call(rbind, Map(data.frame, author_name=test_author_names, file_id=mynames_test))
colnames(test_df) <- c('author_name', 'file_id')
test_df$type <- 'test'
```

```{r, echo=FALSE, include=FALSE}
all_documents <- rbind(all_train, all_test)
```

```{r, echo=FALSE, include=FALSE}
documents_raw <- Corpus(VectorSource(all_documents))
my_documents<- documents_raw %>%
  tm_map(content_transformer(tolower))  %>%             # make everything lowercase
  tm_map(content_transformer(removeNumbers)) %>%        # remove numbers
  tm_map(content_transformer(removePunctuation)) %>%    # remove punctuation
  tm_map(content_transformer(stripWhitespace))
```

```{r, echo=FALSE, include=FALSE}
train_documents_raw <- Corpus(VectorSource(all_train))
train_my_documents <- train_documents_raw %>%
  tm_map(content_transformer(tolower))  %>%             # make everything lowercase
  tm_map(content_transformer(removeNumbers)) %>%        # remove numbers
  tm_map(content_transformer(removePunctuation)) %>%    # remove punctuation
  tm_map(content_transformer(stripWhitespace))         

test_documents_raw <- Corpus(VectorSource(all_test))
test_my_documents <- test_documents_raw %>%
  tm_map(content_transformer(tolower))  %>%             # make everything lowercase
  tm_map(content_transformer(removeNumbers)) %>%        # remove numbers
  tm_map(content_transformer(removePunctuation)) %>%    # remove punctuation
  tm_map(content_transformer(stripWhitespace))         
```

```{r, echo=FALSE, include=FALSE}
#stopwords("en")
#stopwords("SMART")
my_documents <- tm_map(my_documents, content_transformer(removeWords),
                       stopwords("en"))
train_my_documents <- tm_map(train_my_documents, content_transformer(removeWords),
                       stopwords("en"))
test_my_documents <- tm_map(test_my_documents, content_transformer(removeWords),
                       stopwords("en"))
#stem
my_documents <- tm_map(my_documents, content_transformer(stemDocument))
train_my_documents <- tm_map(train_my_documents, content_transformer(stemDocument))
test_my_documents <- tm_map(test_my_documents, content_transformer(stemDocument))
```

```{r, echo=FALSE, include=FALSE}
#all combined 
DTM <- DocumentTermMatrix(my_documents)
DTM <- removeSparseTerms(DTM, 0.95)
tfidf <- weightTfIdf(DTM)
X <- as.matrix(tfidf)
scrub_cols <- which(colSums(X) == 0)
X <- X[,-scrub_cols]
X_df <- data.frame(X)

X_train_all <- X_df[1:2500, ]
X_test_all <- X_df[2501:5000, ]
```

```{r, echo=FALSE, include=FALSE}
#train
DTM_train <- DocumentTermMatrix(train_my_documents)
DTM_train <- removeSparseTerms(DTM_train, 0.95)
tfidf_train <- weightTfIdf(DTM_train)
X_train <- as.matrix(tfidf_train)
#scrub_cols_train <- which(colSums(X_train) == 0)
#X_train <- X_train[,-scrub_cols_train]
X_train_df <- data.frame(X_train)

#test
DTM_test <- DocumentTermMatrix(test_my_documents)
DTM_test <- removeSparseTerms(DTM_test, 0.95)
tfidf_test <- weightTfIdf(DTM_test)
X_test <- as.matrix(tfidf_test)
#scrub_cols_test<- which(colSums(X_test) == 0)
#X_test <- X_test[,-scrub_cols_test]
X_test_df <- data.frame(X_test)

#filter test
test_sub1 <- data.frame(X_test_df[,intersect(colnames(X_test_df),
                                           colnames(X_train_df))])
test_sub2 <- read.table(textConnection(""), col.names = colnames(X_train_df),
                 colClasses = "integer")
X_test_df2 <- rbind.fill(test_sub1, test_sub2)
X_test_df2[is.na(X_test_df2)] <- 0

filtered_test <- X_test_df[ , which(names(X_test_df) %in% colnames(X_train_df))]
filtered_test2 <- rbind.fill(filtered_test, test_sub2)
filtered_test2[is.na(filtered_test2)] <- 0
```

```{r, echo=FALSE, include=FALSE}
y_train_data <- as.factor(train_df$author_name)
y_test_data <- as.factor(test_df$author_name)
```

```{r, echo=FALSE, include=FALSE}
X_train_cosine <- as.matrix(X_train_df)
sim <- X_train_cosine / sqrt(rowSums(X_train_cosine * X_train_cosine))
sim <- sim %*% t(sim)
D_sim_train <- as.dist(1 - sim)

X_test_cosine <- as.matrix(filtered_test2)
sim <- X_test_cosine / sqrt(rowSums(X_test_cosine * X_test_cosine))
sim <- sim %*% t(sim)
D_sim_test <- as.dist(1 - sim)
```

```{r, echo=FALSE, include=FALSE}
#knn model 
pr <- knn(D_sim_train, D_sim_test, cl=y_train_data, k=5)
tab <- table(pr,y_test_data)
knn_acc <- mean(pr == y_test_data)
```

```{r, echo=FALSE, include=FALSE}
#decision tree
X_train_full_df <- cbind(X_train_df, author_name = train_df$author_name)
fit <- rpart(author_name~., data = X_train_full_df, method = 'class')
predict_unseen <- predict(fit, filtered_test2, type = 'class')
dt_acc <- mean(predict_unseen == y_test_data)
```

```{r, echo=FALSE, include=FALSE}
#multinomial naive bayes
mnb <- multinomial_naive_bayes(x = X_train_df, y = y_train_data, laplace = 1/ncol(X_train_df))
nb_pr <- predict(mnb, as.matrix(filtered_test2), type = "class")
nb_acc <- mean(nb_pr == y_test_data)
```

### Data Pre-Processing Explanation

For this task, I first collected the data and did basic prep work by reading in all of the files (train and test), extracting the author names for classification, and cleaning the file names. I first decided to create a tf-idf matrix for the train and test files separately. Before doing that, I first turned the train and test documents (separately) into corpora. For each corpus, I made everything lowercase, removed numbers and punctuation and stripped whitespace. I then removed stop words as these words do not carry semantic meaning or really any helpful information. stop words are needed for grammatical construction, but on their own, they are not helpful. Then, I stemmed the document so that words such as 'running' and 'run' can be treated as the same word. This also helps to reduce the number of features (i.e., words) in our model later on. From there, for each the train and test sets, I created a document term matrix and removed any sparse terms that have a count of zero in 95% of documents. Most likely those do not provide helpful information and only add to the complexity of the model (via the number of features). I then got the weighted tf-idf matrix based on these document term matrices for each the train and test set. 

### Analysis Explanation

For the models, I considered the tf-idf matrix as the dataset (features) for the classification models to predict the author who wrote a given article. Each predictor is a term, and each value, is the tf-idf score of that term in any given document (i.e., the row).

First, I used a KNN classifier to do this. I then created a cosine distance matrix for the tf-idf weights and fit a KNN to training data (cosine distance matrix of the training data tf-idf weights). When applied to the test data (also converted to a cosine distance matrix of the test data), the model achieved an accuracy of 47% with the value of k, nearest neighbors, at 5. If one figures that a human has a 1/50 (or 2%) chance at correctly guessing the author, then this accuracy is much higher than the threshold and outperforms a random human guess. 

The next model I considered was a multinomial naive bayes model. I built the model using the tf-idf weights of the training data and a Laplace  smoothing factor of 1/number of terms. This model outperformed the KNN model at 59%. The Laplace  factor here accounts for a pseudo count of words that we did not seen in the training data, but we did see in the test data documents. In comparison to the KNN model, which did not add a pseudo count , this model outperforms, possibly explained by the inclusion of a pseudo count.

Predicting the author of an article is a difficult classification task. As explained above, when a tf-idf matrix is used in the model, where each feature is a term and each value is a tf-idf score of that term in the document, much better job at naming the author of an article over a random human guess is achieved Extensive data pre-processing techniques including cleaning non-word characters, removing stop words, and stemming words, refine the documents so that a more meaningful and helpful tf-idf matrix is constructed. 





